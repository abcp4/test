{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import utils\n",
    "import data_manager as dm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Cross-Validation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook de validação cruzada da tarefa de qualidade de imagem sismográfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos as informações do dataset, já particionadas em treino-validação-teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = 'data/result_path.json'\n",
    "\n",
    "with open(load_from, 'r') as f:\n",
    "    data_desc = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-carregamos o dataset em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6610/6610 [02:25<00:00, 45.56it/s]\n"
     ]
    }
   ],
   "source": [
    "all_indices = np.arange(len(data_desc['train'] + data_desc['dev'] + data_desc['test']))\n",
    "all_images = dm.load_batch(data_desc, all_indices,partition=all_indices,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos uma imagem para definir as dimensões da entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X[0]\n",
    "img_w = sample.shape[0]\n",
    "img_h = sample.shape[1]\n",
    "n_channels = 1\n",
    "n_classes = len(data_desc['id_to_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos o uso de memória do GPU à 30% da capacidade máxima e uma única gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = tf.ConfigProto(device_count={'GPU':1})\n",
    "# default_config.gpu_options.per_process_gpu_memory_fraction=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue o modelo que usamos para a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import models.busson_model_C_block as my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.architecture_manager' from '/workspace/classificacao_sismo/evaluate/models/architecture_manager.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(my_model)\n",
    "reload(models.architecture_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get information from the model and define the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5 =  (?, 299, 299, 8)\n",
      "conv_aux =  (?, 299, 299, 8)\n",
      "shape:  (?, 299, 299, 8)\n",
      "pooling:  (?, 148, 148, 8)\n",
      "conv5 =  (?, 148, 148, 16)\n",
      "conv_aux =  (?, 148, 148, 16)\n",
      "shape:  (?, 148, 148, 16)\n",
      "pooling:  (?, 72, 72, 16)\n",
      "conv5 =  (?, 72, 72, 32)\n",
      "conv_aux =  (?, 72, 72, 32)\n",
      "shape:  (?, 72, 72, 32)\n",
      "pooling:  (?, 34, 34, 32)\n",
      "conv5 =  (?, 34, 34, 64)\n",
      "conv_aux =  (?, 34, 34, 64)\n",
      "shape:  (?, 34, 34, 64)\n",
      "pooling:  (?, 15, 15, 64)\n"
     ]
    }
   ],
   "source": [
    "model_name = my_model.model_name\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    # image input\n",
    "    x = tf.placeholder(tf.float32, shape=(None, img_h, img_w, n_channels), name='X')\n",
    "    # integer class output\n",
    "    y = tf.placeholder(tf.int64, shape=(None,), name='Y')\n",
    "    # input learning rate\n",
    "    lr_placeholder = tf.placeholder(tf.float32)\n",
    "    \n",
    "# get model from input placeholder variable and number of classes\n",
    "output_logits, pred, model_description = my_model.make_model(x, n_classes, alpha=4)\n",
    "# get loss tensor, accuracy tensor, and optimizer function\n",
    "loss, accuracy, optimizer = my_model.make_model_loss(y, lr_placeholder, output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busson_model\n",
      "----\n",
      "inception network architecture\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print('----')\n",
    "print(model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_f1(CM):\n",
    "    e = 1e-8\n",
    "    TP = np.diag(CM)\n",
    "    P = np.sum(CM,axis=1)\n",
    "    C = np.sum(CM,axis=0)\n",
    "    prec = TP/(P+e)\n",
    "    recall = TP/(C+e)\n",
    "    f1 = 2*prec*recall/(prec+recall+e)\n",
    "    return f1, prec, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de treino, recebe um conjunto de índices de treino, validação e outro de teste. \n",
    "Essa função ajusta o modelo com as imagens de treino por um número máximo de épocas e mede o desempenho na validação ao fim de cada época. Se não houver melhora por 'early_stop_epochs' épocas, o treino para e o teste é avaliado com o melhor modelo de acordo com a validação.\n",
    "\n",
    "Retorna a acurácia no treino e teste, além da matriz de confusão no teste e <i>Recall</i> da última classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_schedule(\n",
    "    x,y,lr_placeholder,\n",
    "    train_indices, dev_indices,test_indices,\n",
    "    lr=0.001,\n",
    "    early_stop_epochs=40,\n",
    "    max_epochs=40,\n",
    "    batch_size=64,\n",
    "    display_freq=100,\n",
    "    normalization=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train a model with input 'x' output 'y' and learning rate 'lr_placholder' for up to 'max_epochs'.\n",
    "    if accuracy on 'dev_indices' does not improve for 'early_stop' epochs, it halts training and evaluates on\n",
    "    'test_indices' with best_current_model\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(dev_indices) == 0:\n",
    "        print('no early stopping, save last model at epoch ', max_epochs)\n",
    "    #Hyper Parameters\n",
    "    logs_path = \"./logs\"  # path to the folder that we want to save the logs for Tensorboard\n",
    "    checkpoint_path = 'checkpoints/'\n",
    "    \n",
    "    train_indices= np.array(train_indices)\n",
    "    dev_indices = np.array(dev_indices)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess = tf.Session(config=default_config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    # Number of training iterations in each epoch\n",
    "\n",
    "    indices = np.arange(len(train_indices))\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    death_counter = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "        lr *= 0.999\n",
    "        np.random.shuffle(indices)\n",
    "        for iteration in range(0,len(indices),batch_size):\n",
    "            idx = indices[iteration:min(iteration+batch_size,len(indices))]\n",
    "            x_batch = X[train_indices[idx]]\n",
    "            y_batch = Y[train_indices[idx]]\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: x_batch, y: y_batch, lr_placeholder:lr}\n",
    "            sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "            \n",
    "        #train accuracy\n",
    "#         mean_train_acc = 0\n",
    "#         train_count = 0\n",
    "\n",
    "#         for iteration in range(0,len(train_indices),batch_size):\n",
    "#             idx = range(iteration,min(len(train_indices),iteration+batch_size))\n",
    "#             x_batch = X[train_indices[idx]]\n",
    "#             y_batch = Y[train_indices[idx]]\n",
    "#             feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "#             loss_train, acc_train = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "#             n = len(y_batch)\n",
    "#             train_count += n\n",
    "#             mean_train_acc += acc_train*n\n",
    "#         mean_train_acc /= train_count\n",
    "#         print('current train acc: ', mean_train_acc)\n",
    "        \n",
    "        #dev accuracy\n",
    "        mean_valid_acc = 0\n",
    "        valid_count = 0\n",
    "        if len(dev_indices) > 0:\n",
    "            for iteration in range(0,len(dev_indices),batch_size):\n",
    "                idx = range(iteration,min(len(dev_indices),iteration+batch_size))\n",
    "                x_batch = X[dev_indices[idx]]\n",
    "                y_batch = Y[dev_indices[idx]]\n",
    "                feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "                loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "                n = len(y_batch)\n",
    "                valid_count += n\n",
    "                mean_valid_acc += acc_valid*n\n",
    "            mean_valid_acc /= valid_count\n",
    "            print('current valid acc: ', mean_valid_acc)\n",
    "            if mean_valid_acc > best_dev_acc:\n",
    "                best_dev_acc = mean_valid_acc\n",
    "                saver.save(sess, checkpoint_path + model_name)\n",
    "                death_counter = 0\n",
    "            else:\n",
    "                death_counter += 1\n",
    "        elif epoch == max_epochs-1:\n",
    "            print('Save last model')\n",
    "            saver.save(sess, checkpoint_path + model_name)\n",
    "\n",
    "        if len(dev_indices) > 0 and death_counter >= early_stop_epochs:\n",
    "            break\n",
    "\n",
    "    saver.restore(sess, checkpoint_path + model_name)\n",
    "        \n",
    "    #train accuracy\n",
    "    mean_train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    for iteration in range(0,len(train_indices),batch_size):\n",
    "        idx = range(iteration,min(len(train_indices),iteration+batch_size))\n",
    "        x_batch = X[train_indices[idx]]\n",
    "        y_batch = Y[train_indices[idx]]\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        loss_train, acc_train = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "        n = len(y_batch)\n",
    "        train_count += n\n",
    "        mean_train_acc += acc_train*n\n",
    "    mean_train_acc /= train_count\n",
    "\n",
    "    #test accuracy\n",
    "    mean_valid_acc = 0\n",
    "    mean_valid_loss = 0 \n",
    "    valid_count = 0\n",
    "\n",
    "    dev_pred = []\n",
    "    dev_target = []\n",
    "    CM = np.zeros((3, 3))\n",
    "    \n",
    "    for iteration in range(0,len(test_indices),batch_size):\n",
    "        idx = range(iteration,min(iteration+batch_size,len(test_indices)))\n",
    "        x_batch = X[test_indices[idx]]\n",
    "        y_batch = Y[test_indices[idx]]\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        valid_pred = sess.run(pred, feed_dict=feed_dict_batch)\n",
    "        \n",
    "        dev_pred.extend(valid_pred)\n",
    "        dev_target.extend(y_batch)\n",
    "        valid_count += len(idx)\n",
    "        \n",
    "    c = 0\n",
    "    for j in range(len(dev_pred)):\n",
    "        if dev_pred[j] == dev_target[j]:\n",
    "            c += 1\n",
    "        CM[dev_target[j],dev_pred[j]] += 1\n",
    "    \n",
    "    class2_recall = CM[1][1]/(np.sum(CM[1])+1e-8)\n",
    "    print(CM)\n",
    "    mean_valid_acc = c/len(test_indices)\n",
    "    cf1, prec, recall = compute_class_f1(CM)\n",
    "\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Training accuracy: {:.01%}'.format(mean_train_acc))\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, mean_valid_loss, mean_valid_acc))\n",
    "    print('F1 score per class (good,bad,ugly): ', cf1)\n",
    "    print('Precision per class: ', prec)\n",
    "    print('Recall per class: ', recall)\n",
    "    print('Mean F1 score: ', np.mean(cf1))\n",
    "    print('Mean Precision: ', np.mean(prec))\n",
    "    print('Mean Recall: ', np.mean(recall))\n",
    "    print('---------------------------------------------------------')\n",
    "\n",
    "    return mean_train_acc, mean_valid_acc, CM, class2_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir configuramos o treino em validação cruzada. Realizamos o experimento apenas na união dos conjuntos de treino, validação e teste do dataset original e particionando em 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_indices = all_indices.copy()\n",
    "np.random.shuffle(shared_indices)\n",
    "n = len(shared_indices)\n",
    "n_folds = 10\n",
    "fold_size = n//n_folds\n",
    "\n",
    "folds = []\n",
    "\n",
    "for i in range(0,n,fold_size):\n",
    "    folds.append(np.array(list(shared_indices[i:min(i+fold_size,n)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos o procedimo de validação cruzada, guardando o desempenho médio e desvio no treino e validação. Assim como cobertura da classe 2 e tempo do procedimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6610, 299, 299, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Fold  1 / 10\n",
      "no early stopping, save last model at epoch  40\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Training epoch: 21\n",
      "Training epoch: 22\n",
      "Training epoch: 23\n",
      "Training epoch: 24\n",
      "Training epoch: 25\n",
      "Training epoch: 26\n",
      "Training epoch: 27\n",
      "Training epoch: 28\n",
      "Training epoch: 29\n",
      "Training epoch: 30\n",
      "Training epoch: 31\n",
      "Training epoch: 32\n",
      "Training epoch: 33\n",
      "Training epoch: 34\n",
      "Training epoch: 35\n",
      "Training epoch: 36\n",
      "Training epoch: 37\n",
      "Training epoch: 38\n",
      "Training epoch: 39\n",
      "Training epoch: 40\n",
      "Save last model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/busson_model\n",
      "[[421.  26.   0.]\n",
      " [  8. 193.   3.]\n",
      " [  0.   6.   4.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 95.9%\n",
      "Epoch: 40, validation loss: 0.00, validation accuracy: 93.5%\n",
      "F1 score per class (good,bad,ugly):  [0.96118721 0.89976689 0.47058823]\n",
      "Precision per class:  [0.94183445 0.94607843 0.4       ]\n",
      "Recall per class:  [0.98135198 0.85777778 0.57142857]\n",
      "Mean F1 score:  0.7771807780750554\n",
      "Mean Precision:  0.7626376276022229\n",
      "Mean Recall:  0.8035194432270018\n",
      "---------------------------------------------------------\n",
      "fold processing time:  1436.1748657226562  s\n",
      "accumulated time:  1436.1754269599915  s\n",
      "Fold  2 / 10\n",
      "no early stopping, save last model at epoch  40\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Training epoch: 21\n",
      "Training epoch: 22\n",
      "Training epoch: 23\n",
      "Training epoch: 24\n",
      "Training epoch: 25\n",
      "Training epoch: 26\n",
      "Training epoch: 27\n",
      "Training epoch: 28\n",
      "Training epoch: 29\n",
      "Training epoch: 30\n",
      "Training epoch: 31\n",
      "Training epoch: 32\n",
      "Training epoch: 33\n",
      "Training epoch: 34\n",
      "Training epoch: 35\n",
      "Training epoch: 36\n",
      "Training epoch: 37\n",
      "Training epoch: 38\n",
      "Training epoch: 39\n",
      "Training epoch: 40\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/busson_model\n",
      "[[432.  11.   0.]\n",
      " [ 15. 185.   8.]\n",
      " [  0.   6.   4.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 96.9%\n",
      "Epoch: 40, validation loss: 0.00, validation accuracy: 93.9%\n",
      "F1 score per class (good,bad,ugly):  [0.97078651 0.90243902 0.36363636]\n",
      "Precision per class:  [0.9751693  0.88942308 0.4       ]\n",
      "Recall per class:  [0.96644295 0.91584158 0.33333333]\n",
      "Mean F1 score:  0.7456206298422062\n",
      "Mean Precision:  0.7548641255613456\n",
      "Mean Recall:  0.7385392900557154\n",
      "---------------------------------------------------------\n",
      "fold processing time:  1395.3618080615997  s\n",
      "accumulated time:  2831.5375578403473  s\n",
      "Fold  3 / 10\n",
      "no early stopping, save last model at epoch  40\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Training epoch: 21\n",
      "Training epoch: 22\n",
      "Training epoch: 23\n",
      "Training epoch: 24\n",
      "Training epoch: 25\n",
      "Training epoch: 26\n",
      "Training epoch: 27\n",
      "Training epoch: 28\n",
      "Training epoch: 29\n",
      "Training epoch: 30\n",
      "Training epoch: 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-656b57b85c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mearly_stop_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         normalization=True,batch_size=batch_size,lr=learning_rate))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfold_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-833de906277e>\u001b[0m in \u001b[0;36mtrain_schedule\u001b[0;34m(x, y, lr_placeholder, train_indices, dev_indices, test_indices, lr, early_stop_epochs, max_epochs, batch_size, display_freq, normalization)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mfeed_dict_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#train accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 40\n",
    "early_stop_epochs=0\n",
    "batch_size = 64\n",
    "# batch_size = 16\n",
    "# learning_rate = (0.001*batch_size/64)\n",
    "learning_rate = 0.0001\n",
    "print(learning_rate)\n",
    "\n",
    "start = time.time()\n",
    "td_results = []\n",
    "findices = list(np.arange(n_folds))\n",
    "for i in range(n_folds):\n",
    "    print('Fold ', (i+1), '/',n_folds)\n",
    "    fold_start = time.time()\n",
    "    findices = utils.rotate_list(findices,1)\n",
    "    train_folds = np.concatenate(np.array(folds)[findices[1:]])\n",
    "#     dev_fold = folds[findices[1]]\n",
    "    dev_fold = []\n",
    "    test_fold = folds[findices[0]]\n",
    "    \n",
    "    td_results.append(train_schedule(\n",
    "        x,y,lr_placeholder,\n",
    "        train_folds, dev_fold,test_fold,\n",
    "        max_epochs=epochs,\n",
    "        early_stop_epochs=early_stop_epochs,\n",
    "        normalization=True,batch_size=batch_size,lr=learning_rate))\n",
    "    \n",
    "    fold_end = time.time()\n",
    "    fold_time = fold_end-fold_start\n",
    "    print('fold processing time: ', fold_time, ' s')\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print('accumulated time: ', elapsed, ' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos os resultados em listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [x[0] for x in td_results]\n",
    "dev_acc = [x[1] for x in td_results]\n",
    "c2_rec = [x[3] for x in td_results]\n",
    "dev_cm = [x[2] for x in td_results]\n",
    "class_f1 = [compute_class_f1(x) for x in dev_cm]\n",
    "mean_f1 = [np.mean(x) for x in class_f1]\n",
    "ugly_f1 = [x[2] for x in class_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computamos a média e desvio de cada valor de interesse, normalizada pela raiz do número de folds.\n",
    "Ou seja, o erro padrão da estimativa\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Standard_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean_acc = np.mean(train_acc)\n",
    "factor = np.sqrt(n_folds)\n",
    "t_dp_acc = np.std(train_acc)/factor\n",
    "\n",
    "d_mean_acc = np.mean(dev_acc)\n",
    "d_dp_acc = np.std(dev_acc)/factor\n",
    "\n",
    "c2_rec_mean = np.mean(c2_rec)\n",
    "c2_rec_dp = np.std(c2_rec)/factor\n",
    "\n",
    "d_mean_f1 = np.mean(mean_f1)\n",
    "d_dp_f1 = np.std(mean_f1)/factor\n",
    "\n",
    "d_mean_uf1 = np.mean(ugly_f1)\n",
    "d_dp_uf1 = np.std(ugly_f1)/factor\n",
    "\n",
    "elapsed_time = elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguem os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9572617246495874  +/-  0.0015384984298314538\n",
      "Dev accuracy:  0.9421331316187594  +/-  0.0016701514497594282\n",
      "Ugly recall:  0.8906242612661885  +/-  0.012316344317967787\n",
      "Mean F1-score 0.7688119873988428  +/-  0.01669173875317283\n",
      "Ugly F1-score 0.43180961530005385 +/- 0.05002627616401592\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', t_mean_acc, ' +/- ', t_dp_acc)\n",
    "print('Dev accuracy: ', d_mean_acc, ' +/- ', d_dp_acc)\n",
    "print('Ugly recall: ', c2_rec_mean, ' +/- ', c2_rec_dp)\n",
    "print('Mean F1-score', d_mean_f1, ' +/- ', d_dp_f1)\n",
    "print('Ugly F1-score', d_mean_uf1, '+/-', d_dp_uf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também salvamos a configuração experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_template = \"\"\"\n",
    "{} fold cross-validation\n",
    "initial learning rate = {}\n",
    "Adam optimizer with decaying learning rate\n",
    "batch_size = {}\n",
    "max_epochs_per_fold = {}\n",
    "early_stop_epochs = {}\n",
    "\"\"\"\n",
    "\n",
    "experiment_description = experiment_template.format(\n",
    "    n_folds,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    early_stop_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvamos os reultados em um arquivo descrevendo o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {\n",
    "    'model_name':model_name,\n",
    "    'model_description':model_description,\n",
    "    'experiment_description':experiment_description,\n",
    "    'train_mean_acc':t_mean_acc,\n",
    "    'train_dp_acc':t_dp_acc,\n",
    "    'dev_mean_acc':d_mean_acc,\n",
    "    'dev_dp_acc':d_dp_acc,\n",
    "    'ugly_mean_recall':c2_rec_mean,\n",
    "    'ugly_dp_recall':c2_rec_dp,\n",
    "    'dev_mean_f1':d_mean_f1,\n",
    "    'dev_dp_f1':d_dp_f1,\n",
    "    'dev_mean_ugly_f1':d_mean_uf1,\n",
    "    'dev_dp_ugly_f1':d_dp_uf1,\n",
    "    'train_acc_list':train_acc,\n",
    "    'dev_acc_list':dev_acc,\n",
    "    'ugly_recall_list':c2_rec,\n",
    "    'elapsed_time':elapsed_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_stamp = model_name + '_' + str(time.time())\n",
    "result_path = 'results/' + model_stamp + '.json'\n",
    "with open(result_path, 'w') as f:\n",
    "    json.dump(dict_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saved at: ', result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
