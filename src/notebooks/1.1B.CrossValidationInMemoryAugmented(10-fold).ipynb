{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import utils\n",
    "import data_manager as dm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Cross-Validation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook de validação cruzada da tarefa de qualidade de imagem sismográfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos as informações do dataset, já particionadas em treino-validação-teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = 'data/dataset_augmented.json'\n",
    "\n",
    "with open(load_from, 'r') as f:\n",
    "    data_desc = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-carregamos o dataset em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(data_desc['train'] + data_desc['dev'] + data_desc['test']))\n",
    "r = dm.load_input_aug(data_desc,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 299, 1)\n",
      "(299, 299, 1)\n",
      "(299, 299, 1)\n",
      "(299, 299, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x.shape) for x in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dm.load_batch_aug(data_desc,[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 299, 299, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(data_desc['train'] + data_desc['dev'] + data_desc['test']))\n",
    "all_images = dm.load_batch(data_desc, all_indices,partition=all_indices,normalize=True)\n",
    "# augmented_images = dm.load_aug_batch(data_desc, all_indices, partition=all_indices,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos uma imagem para definir as dimensões da entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X[0]\n",
    "img_w = sample.shape[0]\n",
    "img_h = sample.shape[1]\n",
    "n_channels = 1\n",
    "n_classes = len(data_desc['id_to_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos o uso de memória do GPU à 30% da capacidade máxima e uma única gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = tf.ConfigProto(device_count={'GPU':1})\n",
    "default_config.gpu_options.per_process_gpu_memory_fraction=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue o modelo que usamos para a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.rafa_model as my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.rafa_model' from '/home/rafael/git/learngeo/classificacao_sismo/evaluate/models/rafa_model.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get information from the model and define the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3872\n",
      "247808\n",
      "991232\n",
      "3964928\n",
      "Tensor(\"batch_normalization_4/FusedBatchNorm:0\", shape=(?, 1, 1, 256), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(?, 256), dtype=float32)\n",
      "32\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "model_name = my_model.model_name\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    # image input\n",
    "    x = tf.placeholder(tf.float32, shape=(None, img_h, img_w, n_channels), name='X')\n",
    "    # integer class output\n",
    "    y = tf.placeholder(tf.int64, shape=(None,), name='Y')\n",
    "    # input learning rate\n",
    "    lr_placeholder = tf.placeholder(tf.float32)\n",
    "    \n",
    "# get model from input placeholder variable and number of classes\n",
    "output_logits, pred, model_description = my_model.make_model(x, n_classes)\n",
    "# get loss tensor, accuracy tensor, and optimizer function\n",
    "loss, accuracy, optimizer = my_model.make_model_loss(y, lr_placeholder, output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rafa_model\n",
      "----\n",
      "two layers convnet, first layer with 32 kernels of size 11 and stride 1\n",
      "followed by 64 kernels of size 11 and stride 2\n",
      "followed by 128 kernels of size 11 and stride 3\n",
      "followed by 256 kernels of size 11 and stride 3\n",
      "each of which followed by max-pooling\n",
      "a fully connected layer of size 32\n",
      "and softmax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print('----')\n",
    "print(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de treino, recebe um conjunto de índices de treino, validação e outro de teste. \n",
    "Essa função ajusta o modelo com as imagens de treino por um número máximo de épocas e mede o desempenho na validação ao fim de cada época. Se não houver melhora por 'early_stop_epochs' épocas, o treino para e o teste é avaliado com o melhor modelo de acordo com a validação.\n",
    "\n",
    "Retorna a acurácia no treino e teste, além da matriz de confusão no teste e <i>Recall</i> da última classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_schedule(\n",
    "    x,y,lr_placeholder,\n",
    "    train_indices, dev_indices,test_indices,\n",
    "    lr=0.001,\n",
    "    early_stop_epochs=40,\n",
    "    max_epochs=40,\n",
    "    batch_size=64,\n",
    "    display_freq=100,\n",
    "    normalization=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train a model with input 'x' output 'y' and learning rate 'lr_placholder' for up to 'max_epochs'.\n",
    "    if accuracy on 'dev_indices' does not improve for 'early_stop' epochs, it halts training and evaluates on\n",
    "    'test_indices' with best_current_model\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(dev_indices) == 0:\n",
    "        print('no early stopping, save last model at epoch ', max_epochs)\n",
    "    #Hyper Parameters\n",
    "    logs_path = \"./logs\"  # path to the folder that we want to save the logs for Tensorboard\n",
    "    checkpoint_path = 'checkpoints/'\n",
    "    \n",
    "    train_indices= np.array(train_indices)\n",
    "    dev_indices = np.array(dev_indices)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess = tf.Session(config=default_config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    # Number of training iterations in each epoch\n",
    "\n",
    "    indices = np.arange(len(train_indices))\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    death_counter = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "        lr *= 0.999\n",
    "        np.random.shuffle(indices)\n",
    "        for iteration in range(0,len(indices),batch_size):\n",
    "            idx = indices[iteration:min(iteration+batch_size,len(indices))]\n",
    "            x_batch = X[train_indices[idx]]\n",
    "            y_batch = Y[train_indices[idx]]\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: x_batch, y: y_batch, lr_placeholder:lr}\n",
    "            sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "            \n",
    "        #train accuracy\n",
    "#         mean_train_acc = 0\n",
    "#         train_count = 0\n",
    "\n",
    "#         for iteration in range(0,len(train_indices),batch_size):\n",
    "#             idx = range(iteration,min(len(train_indices),iteration+batch_size))\n",
    "#             x_batch = X[train_indices[idx]]\n",
    "#             y_batch = Y[train_indices[idx]]\n",
    "#             feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "#             loss_train, acc_train = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "#             n = len(y_batch)\n",
    "#             train_count += n\n",
    "#             mean_train_acc += acc_train*n\n",
    "#         mean_train_acc /= train_count\n",
    "#         print('current train acc: ', mean_train_acc)\n",
    "        \n",
    "        #dev accuracy\n",
    "        mean_valid_acc = 0\n",
    "        valid_count = 0\n",
    "        if len(dev_indices) > 0:\n",
    "            for iteration in range(0,len(dev_indices),batch_size):\n",
    "                idx = range(iteration,min(len(dev_indices),iteration+batch_size))\n",
    "                x_batch = X[dev_indices[idx]]\n",
    "                y_batch = Y[dev_indices[idx]]\n",
    "                feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "                loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "                n = len(y_batch)\n",
    "                valid_count += n\n",
    "                mean_valid_acc += acc_valid*n\n",
    "            mean_valid_acc /= valid_count\n",
    "            print('current valid acc: ', mean_valid_acc)\n",
    "            if mean_valid_acc > best_dev_acc:\n",
    "                best_dev_acc = mean_valid_acc\n",
    "                saver.save(sess, checkpoint_path + model_name)\n",
    "                death_counter = 0\n",
    "            else:\n",
    "                death_counter += 1\n",
    "        elif epoch == max_epochs-1:\n",
    "            print('Save last model')\n",
    "            saver.save(sess, checkpoint_path + model_name)\n",
    "\n",
    "        if len(dev_indices) > 0 and death_counter >= early_stop_epochs:\n",
    "            break\n",
    "\n",
    "    saver.restore(sess, checkpoint_path + model_name)\n",
    "        \n",
    "    #train accuracy\n",
    "    mean_train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    for iteration in range(0,len(train_indices),batch_size):\n",
    "        idx = range(iteration,min(len(train_indices),iteration+batch_size))\n",
    "        x_batch = X[train_indices[idx]]\n",
    "        y_batch = Y[train_indices[idx]]\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        loss_train, acc_train = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "        n = len(y_batch)\n",
    "        train_count += n\n",
    "        mean_train_acc += acc_train*n\n",
    "    mean_train_acc /= train_count\n",
    "\n",
    "    #test accuracy\n",
    "    mean_valid_acc = 0\n",
    "    mean_valid_loss = 0 \n",
    "    valid_count = 0\n",
    "\n",
    "    dev_pred = []\n",
    "    dev_target = []\n",
    "    CM = np.zeros((3, 3))\n",
    "    \n",
    "    for iteration in range(0,len(test_indices),batch_size):\n",
    "        idx = range(iteration,min(iteration+batch_size,len(test_indices)))\n",
    "        x_batch = X[test_indices[idx]]\n",
    "        y_batch = Y[test_indices[idx]]\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        valid_pred = sess.run(pred, feed_dict=feed_dict_batch)\n",
    "        \n",
    "        dev_pred.extend(valid_pred)\n",
    "        dev_target.extend(y_batch)\n",
    "        valid_count += len(idx)\n",
    "        \n",
    "    c = 0\n",
    "    for j in range(len(dev_pred)):\n",
    "        if dev_pred[j] == dev_target[j]:\n",
    "            c += 1\n",
    "        CM[dev_target[j],dev_pred[j]] += 1\n",
    "    \n",
    "    class2_recall = CM[1][1]/(np.sum(CM[1])+1e-8)\n",
    "    print(CM)\n",
    "    mean_valid_acc = c/len(test_indices)\n",
    "\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Training accuracy: {:.01%}'.format(mean_train_acc))\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, mean_valid_loss, mean_valid_acc))\n",
    "    print('---------------------------------------------------------')\n",
    "\n",
    "    return mean_train_acc, mean_valid_acc, CM, class2_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir configuramos o treino em validação cruzada. Realizamos o experimento apenas na união dos conjuntos de treino, validação e teste do dataset original e particionando em 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_indices = all_indices\n",
    "np.random.shuffle(shared_indices)\n",
    "n = len(shared_indices)\n",
    "n_folds = 10\n",
    "fold_size = n//n_folds\n",
    "\n",
    "folds = []\n",
    "\n",
    "for i in range(0,n,fold_size):\n",
    "    folds.append(np.array(list(shared_indices[i:min(i+fold_size,n)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos o procedimo de validação cruzada, guardando o desempenho médio e desvio no treino e validação. Assim como cobertura da classe 2 e tempo do procedimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Fold  1 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[78.  4.  0.]\n",
      " [10. 28.  4.]\n",
      " [ 0.  7.  7.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 95.9%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 81.9%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  127.64417958259583  s\n",
      "accumulated time:  127.64483404159546  s\n",
      "Fold  2 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[85.  4.  0.]\n",
      " [ 9. 22.  2.]\n",
      " [ 0.  4. 12.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 95.4%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 86.2%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.1070158481598  s\n",
      "accumulated time:  258.75320649147034  s\n",
      "Fold  3 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[76.  5.  0.]\n",
      " [ 4. 34.  3.]\n",
      " [ 0.  2. 14.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 96.9%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 89.9%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.10008120536804  s\n",
      "accumulated time:  389.85384225845337  s\n",
      "Fold  4 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[77.  5.  0.]\n",
      " [ 8. 37.  3.]\n",
      " [ 0.  3.  5.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 95.6%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 86.2%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.24299812316895  s\n",
      "accumulated time:  521.0971212387085  s\n",
      "Fold  5 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[87.  5.  0.]\n",
      " [ 4. 29.  1.]\n",
      " [ 0.  2. 10.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 96.4%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 91.3%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.54651546478271  s\n",
      "accumulated time:  652.6450426578522  s\n",
      "Fold  6 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[85.  5.  0.]\n",
      " [ 9. 28.  3.]\n",
      " [ 0.  0.  8.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 96.2%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 87.7%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.6715886592865  s\n",
      "accumulated time:  784.317675113678  s\n",
      "Fold  7 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[81.  4.  0.]\n",
      " [ 4. 33.  3.]\n",
      " [ 0.  1. 12.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 98.0%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 91.3%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.5934715270996  s\n",
      "accumulated time:  915.9119684696198  s\n",
      "Fold  8 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[78.  2.  0.]\n",
      " [ 7. 35.  3.]\n",
      " [ 0.  1. 12.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 92.6%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 90.6%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.27348041534424  s\n",
      "accumulated time:  1047.185712814331  s\n",
      "Fold  9 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[73.  2.  0.]\n",
      " [13. 29.  5.]\n",
      " [ 0.  1. 15.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 92.5%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 84.8%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.72826480865479  s\n",
      "accumulated time:  1178.916740179062  s\n",
      "Fold  10 / 10\n",
      "no early stopping, save last model at epoch  20\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n",
      "Training epoch: 10\n",
      "Training epoch: 11\n",
      "Training epoch: 12\n",
      "Training epoch: 13\n",
      "Training epoch: 14\n",
      "Training epoch: 15\n",
      "Training epoch: 16\n",
      "Training epoch: 17\n",
      "Training epoch: 18\n",
      "Training epoch: 19\n",
      "Training epoch: 20\n",
      "Save last model\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/rafa_model\n",
      "[[77. 12.  0.]\n",
      " [ 1. 25.  6.]\n",
      " [ 0.  3. 14.]]\n",
      "---------------------------------------------------------\n",
      "Training accuracy: 94.4%\n",
      "Epoch: 20, validation loss: 0.00, validation accuracy: 84.1%\n",
      "---------------------------------------------------------\n",
      "fold processing time:  131.7094612121582  s\n",
      "accumulated time:  1310.627830505371  s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 20\n",
    "early_stop_epochs=0\n",
    "# batch_size = 64\n",
    "batch_size = 16\n",
    "# learning_rate = (0.001*batch_size/64)\n",
    "learning_rate = 0.0001\n",
    "print(learning_rate)\n",
    "\n",
    "start = time.time()\n",
    "td_results = []\n",
    "findices = list(np.arange(n_folds))\n",
    "for i in range(n_folds):\n",
    "    print('Fold ', (i+1), '/',n_folds)\n",
    "    fold_start = time.time()\n",
    "    findices = utils.rotate_list(findices,1)\n",
    "    train_folds = np.concatenate(np.array(folds)[findices[1:]])\n",
    "#     dev_fold = folds[findices[1]]\n",
    "    dev_fold = []\n",
    "    test_fold = folds[findices[0]]\n",
    "    \n",
    "    td_results.append(train_schedule(\n",
    "        x,y,lr_placeholder,\n",
    "        train_folds, dev_fold,test_fold,\n",
    "        max_epochs=epochs,\n",
    "        early_stop_epochs=early_stop_epochs,\n",
    "        normalization=True,batch_size=batch_size,lr=learning_rate))\n",
    "    fold_end = time.time()\n",
    "    fold_time = fold_end-fold_start\n",
    "    print('fold processing time: ', fold_time, ' s')\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print('accumulated time: ', elapsed, ' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos os resultados em listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [x[0] for x in td_results]\n",
    "dev_acc = [x[1] for x in td_results]\n",
    "c2_rec = [x[3] for x in td_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computamos a média e desvio de cada valor de interesse, normalizada pela raiz do número de folds.\n",
    "Ou seja, o erro padrão da estimativa\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Standard_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean_acc = np.mean(train_acc)\n",
    "t_dp_acc = np.std(train_acc)/np.sqrt(n_folds)\n",
    "\n",
    "d_mean_acc = np.mean(dev_acc)\n",
    "d_dp_acc = np.std(dev_acc)/np.sqrt(n_folds)\n",
    "\n",
    "c2_rec_mean = np.mean(c2_rec)\n",
    "c2_rec_dp = np.std(c2_rec)/np.sqrt(n_folds)\n",
    "\n",
    "elapsed_time = elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguem os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9539452495494327  +/-  0.005300328845631292\n",
      "Dev accuracy:  0.8739130434782607  +/-  0.009893361255580295\n",
      "Ugly recall:  0.7487425188289727  +/-  0.02423715338041163\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', t_mean_acc, ' +/- ', t_dp_acc)\n",
    "print('Dev accuracy: ', d_mean_acc, ' +/- ', d_dp_acc)\n",
    "print('Ugly recall: ', c2_rec_mean, ' +/- ', c2_rec_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também salvamos a configuração experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_template = \"\"\"\n",
    "{} fold cross-validation\n",
    "initial learning rate = {}\n",
    "Adam optimizer with decaying learning rate\n",
    "batch_size = {}\n",
    "max_epochs_per_fold = {}\n",
    "early_stop_epochs = {}\n",
    "\"\"\"\n",
    "\n",
    "experiment_description = experiment_template.format(\n",
    "    n_folds,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    early_stop_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 fold cross-validation\n",
      "initial learning rate = 0.0001\n",
      "Adam optimizer with decaying learning rate\n",
      "batch_size = 16\n",
      "max_epochs_per_fold = 20\n",
      "early_stop_epochs = 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvamos os reultados em um arquivo descrevendo o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {\n",
    "    'model_name':model_name,\n",
    "    'model_description':model_description,\n",
    "    'experiment_description':experiment_description,\n",
    "    'train_mean_acc':t_mean_acc,\n",
    "    'train_dp_acc':t_dp_acc,\n",
    "    'dev_mean_acc':d_mean_acc,\n",
    "    'dev_dp_acc':d_dp_acc,\n",
    "    'ugly_mean_recall':c2_rec_mean,\n",
    "    'ugly_dp_recall':c2_rec_dp,\n",
    "    'train_acc_list':train_acc,\n",
    "    'dev_acc_list':dev_acc,\n",
    "    'ugly_recall_list':c2_rec,\n",
    "    'elapsed_time':elapsed_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'rafa_model',\n",
       " 'model_description': 'two layers convnet, first layer with 32 kernels of size 11 and stride 1\\nfollowed by 64 kernels of size 11 and stride 2\\nfollowed by 128 kernels of size 11 and stride 3\\nfollowed by 256 kernels of size 11 and stride 3\\neach of which followed by max-pooling\\na fully connected layer of size 32\\nand softmax\\n',\n",
       " 'experiment_description': '\\n10 fold cross-validation\\ninitial learning rate = 0.0001\\nAdam optimizer with decaying learning rate\\nbatch_size = 16\\nmax_epochs_per_fold = 20\\nearly_stop_epochs = 0\\n',\n",
       " 'train_mean_acc': 0.9539452495494327,\n",
       " 'train_dp_acc': 0.005300328845631292,\n",
       " 'dev_mean_acc': 0.8739130434782607,\n",
       " 'dev_dp_acc': 0.009893361255580295,\n",
       " 'ugly_mean_recall': 0.7487425188289727,\n",
       " 'ugly_dp_recall': 0.02423715338041163,\n",
       " 'train_acc_list': [0.9589371980676329,\n",
       "  0.9541062801932367,\n",
       "  0.9694041867954911,\n",
       "  0.9557165861513688,\n",
       "  0.9637681159420289,\n",
       "  0.9621578097919335,\n",
       "  0.979871175331386,\n",
       "  0.9259259259259259,\n",
       "  0.9251207727548965,\n",
       "  0.9444444445404262],\n",
       " 'dev_acc_list': [0.8188405797101449,\n",
       "  0.8623188405797102,\n",
       "  0.8985507246376812,\n",
       "  0.8623188405797102,\n",
       "  0.9130434782608695,\n",
       "  0.8768115942028986,\n",
       "  0.9130434782608695,\n",
       "  0.9057971014492754,\n",
       "  0.8478260869565217,\n",
       "  0.8405797101449275],\n",
       " 'ugly_recall_list': [0.6666666665079365,\n",
       "  0.6666666664646465,\n",
       "  0.8292682924806662,\n",
       "  0.770833333172743,\n",
       "  0.8529411762197232,\n",
       "  0.6999999998249999,\n",
       "  0.8249999997937499,\n",
       "  0.7777777776049383,\n",
       "  0.6170212764644636,\n",
       "  0.7812499997558594],\n",
       " 'elapsed_time': 1310.627830505371}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_stamp = model_name + '_' + str(time.time())\n",
    "result_path = 'results/' + model_stamp + '.json'\n",
    "with open(result_path, 'w') as f:\n",
    "    json.dump(dict_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at:  results/rafa_model_1544133152.131615.json\n"
     ]
    }
   ],
   "source": [
    "print('saved at: ', result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
